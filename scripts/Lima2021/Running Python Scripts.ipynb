{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d88a64d6-823f-44b7-a074-c820e6ecd189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dea6c77d-41d5-429b-ae49-eeeaa4ee0a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [01:08<00:00,  3.59s/it]\n"
     ]
    }
   ],
   "source": [
    "%run loading_aggregating_raw_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2023d692-a0da-4822-8c20-6ca18c0449ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly_Distributed_Observations_100\n",
      "Land_Only_Distributed_Observations_100\n"
     ]
    }
   ],
   "source": [
    "%run simulating_observations.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a6113ce-0f96-45ac-b2c7-7905f7ab6c08",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-13 15:31:16.432017: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "sample: 100%|██████████| 3000/3000 [04:14<00:00, 11.81it/s, 63 steps of size 1.09e-01. acc. prob=0.90] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                 mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "         a0     -4.30      0.22     -4.29     -4.66     -3.93   2762.27      1.00\n",
      "         a1      4.05      0.29      4.05      3.57      4.51   2780.05      1.00\n",
      " alpha[0,0]      0.65      0.07      0.64      0.53      0.76   4424.95      1.00\n",
      " alpha[0,1]      0.67      0.05      0.67      0.58      0.75   3941.07      1.00\n",
      " alpha[0,2]      0.69      0.05      0.69      0.60      0.77   3556.01      1.00\n",
      " alpha[0,3]      0.73      0.06      0.73      0.63      0.83   3810.20      1.00\n",
      " alpha[0,4]      0.82      0.07      0.82      0.70      0.93   4852.63      1.00\n",
      " alpha[0,5]      0.58      0.06      0.58      0.49      0.67   5186.60      1.00\n",
      " alpha[0,6]      0.69      0.06      0.68      0.60      0.78   3736.62      1.00\n",
      " alpha[0,7]      0.92      0.07      0.91      0.81      1.03   4470.58      1.00\n",
      " alpha[0,8]      0.55      0.05      0.55      0.47      0.63   4033.66      1.00\n",
      " alpha[0,9]      1.00      0.07      1.00      0.88      1.12   5090.89      1.00\n",
      "alpha[0,10]      1.03      0.08      1.02      0.90      1.16   3342.34      1.00\n",
      "alpha[0,11]      0.96      0.07      0.96      0.85      1.07   4396.95      1.00\n",
      "alpha[0,12]      0.84      0.06      0.84      0.74      0.94   4661.92      1.00\n",
      "alpha[0,13]      0.61      0.04      0.61      0.54      0.68   4384.02      1.00\n",
      "alpha[0,14]      0.67      0.05      0.67      0.59      0.75   4654.50      1.00\n",
      "alpha[0,15]      0.84      0.06      0.84      0.73      0.93   4784.39      1.00\n",
      "alpha[0,16]      0.74      0.05      0.73      0.65      0.82   4820.16      1.00\n",
      "alpha[0,17]      0.96      0.08      0.96      0.84      1.10   5547.00      1.00\n",
      "alpha[0,18]      0.91      0.07      0.91      0.80      1.03   4563.19      1.00\n",
      "alpha[0,19]      0.76      0.05      0.76      0.67      0.85   4944.68      1.00\n",
      "alpha[0,20]      0.63      0.04      0.63      0.56      0.70   3453.80      1.00\n",
      "alpha[0,21]      0.88      0.06      0.88      0.78      0.98   4131.51      1.00\n",
      "alpha[0,22]      0.87      0.06      0.87      0.77      0.97   4159.92      1.00\n",
      "alpha[0,23]      0.67      0.04      0.67      0.60      0.74   4214.91      1.00\n",
      "alpha[0,24]      0.91      0.07      0.91      0.80      1.01   4982.03      1.00\n",
      "alpha[0,25]      0.72      0.05      0.72      0.65      0.80   4136.57      1.00\n",
      "alpha[0,26]      0.82      0.05      0.82      0.73      0.90   4134.63      1.00\n",
      "alpha[0,27]      0.81      0.06      0.81      0.71      0.90   5098.88      1.00\n",
      "alpha[0,28]      0.79      0.05      0.79      0.70      0.87   4650.14      1.00\n",
      "alpha[0,29]      0.75      0.05      0.75      0.67      0.83   4360.17      1.00\n",
      "alpha[0,30]      0.79      0.05      0.78      0.70      0.86   4830.58      1.00\n",
      "alpha[0,31]      0.82      0.05      0.82      0.75      0.90   5026.10      1.00\n",
      "alpha[0,32]      0.83      0.05      0.83      0.76      0.92   4786.50      1.00\n",
      "alpha[0,33]      0.74      0.05      0.74      0.66      0.81   5209.16      1.00\n",
      "alpha[0,34]      0.85      0.05      0.85      0.77      0.94   4052.00      1.00\n",
      "alpha[0,35]      0.85      0.05      0.85      0.76      0.92   4356.49      1.00\n",
      "alpha[0,36]      0.85      0.05      0.85      0.77      0.93   4226.82      1.00\n",
      "alpha[0,37]      0.73      0.04      0.73      0.66      0.80   4389.19      1.00\n",
      "alpha[0,38]      0.78      0.05      0.77      0.70      0.86   4606.67      1.00\n",
      "alpha[0,39]      0.63      0.05      0.63      0.54      0.70   4362.56      1.00\n",
      "alpha[0,40]      0.76      0.05      0.76      0.69      0.84   4463.38      1.00\n",
      "alpha[0,41]      0.92      0.05      0.91      0.82      1.00   3411.71      1.00\n",
      "alpha[0,42]      0.74      0.05      0.74      0.66      0.82   4628.67      1.00\n",
      "alpha[0,43]      0.86      0.05      0.86      0.77      0.94   4063.60      1.00\n",
      "alpha[0,44]      0.93      0.05      0.93      0.84      1.01   4608.05      1.00\n",
      "alpha[0,45]      0.78      0.06      0.78      0.68      0.89   3611.23      1.00\n",
      "alpha[0,46]      0.93      0.05      0.93      0.85      1.01   4059.83      1.00\n",
      "alpha[0,47]      0.91      0.06      0.91      0.81      1.00   5250.78      1.00\n",
      "alpha[0,48]      0.85      0.06      0.85      0.76      0.95   4364.21      1.00\n",
      "alpha[0,49]      0.97      0.06      0.96      0.87      1.06   5003.12      1.00\n",
      "alpha[0,50]      0.83      0.05      0.83      0.75      0.92   3342.63      1.00\n",
      "alpha[0,51]      0.83      0.05      0.83      0.75      0.90   4365.73      1.00\n",
      "alpha[0,52]      0.71      0.05      0.71      0.63      0.78   3652.58      1.00\n",
      "alpha[0,53]      0.75      0.05      0.75      0.68      0.83   5276.66      1.00\n",
      "alpha[0,54]      0.79      0.05      0.79      0.70      0.87   4020.08      1.00\n",
      "alpha[0,55]      0.80      0.06      0.79      0.70      0.89   4160.49      1.00\n",
      "alpha[0,56]      0.71      0.05      0.71      0.63      0.79   7379.33      1.00\n",
      "alpha[0,57]      0.68      0.04      0.68      0.62      0.75   4235.20      1.00\n",
      "alpha[0,58]      0.69      0.04      0.69      0.61      0.76   3357.75      1.00\n",
      "alpha[0,59]      0.69      0.04      0.69      0.62      0.77   4632.74      1.00\n",
      "alpha[0,60]      0.74      0.05      0.74      0.66      0.82   5663.82      1.00\n",
      "alpha[0,61]      0.71      0.05      0.71      0.62      0.79   4670.28      1.00\n",
      "alpha[0,62]      0.81      0.04      0.81      0.74      0.88   3938.23      1.00\n",
      "alpha[0,63]      0.58      0.03      0.58      0.52      0.63   4149.31      1.00\n",
      "alpha[0,64]      0.89      0.05      0.89      0.79      0.97   5317.21      1.00\n",
      "alpha[0,65]      0.80      0.05      0.80      0.71      0.88   3359.59      1.00\n",
      "alpha[0,66]      0.66      0.04      0.66      0.60      0.72   3302.91      1.00\n",
      "alpha[0,67]      0.95      0.05      0.95      0.87      1.03   4577.90      1.00\n",
      "alpha[0,68]      0.90      0.06      0.90      0.80      1.00   4094.02      1.00\n",
      "alpha[0,69]      0.73      0.04      0.73      0.65      0.80   4843.82      1.00\n",
      "alpha[0,70]      0.76      0.05      0.76      0.68      0.85   4663.33      1.00\n",
      "alpha[0,71]      0.80      0.05      0.80      0.71      0.88   3862.83      1.00\n",
      "alpha[0,72]      0.95      0.07      0.95      0.85      1.06   4100.78      1.00\n",
      "alpha[0,73]      0.80      0.06      0.80      0.69      0.90   4139.06      1.00\n",
      "alpha[0,74]      0.97      0.05      0.97      0.88      1.05   4127.86      1.00\n",
      "alpha[0,75]      0.81      0.05      0.81      0.72      0.90   3823.49      1.00\n",
      "alpha[0,76]      0.74      0.05      0.74      0.65      0.83   4560.98      1.00\n",
      "alpha[0,77]      0.84      0.05      0.84      0.77      0.93   5196.08      1.00\n",
      "alpha[0,78]      0.71      0.05      0.71      0.61      0.79   6001.72      1.00\n",
      "alpha[0,79]      0.64      0.04      0.64      0.57      0.71   3799.36      1.00\n",
      "alpha[0,80]      0.80      0.05      0.80      0.72      0.88   4713.86      1.00\n",
      "alpha[0,81]      0.69      0.04      0.69      0.62      0.76   5908.93      1.00\n",
      "alpha[0,82]      0.79      0.05      0.79      0.73      0.88   4514.67      1.00\n",
      "alpha[0,83]      0.78      0.05      0.78      0.71      0.85   4612.64      1.00\n",
      "alpha[0,84]      0.64      0.04      0.64      0.57      0.70   3794.34      1.00\n",
      "alpha[0,85]      0.66      0.04      0.66      0.59      0.73   3708.73      1.00\n",
      "alpha[0,86]      0.91      0.05      0.91      0.83      0.99   4190.28      1.00\n",
      "alpha[0,87]      0.57      0.04      0.57      0.51      0.63   4620.24      1.00\n",
      "alpha[0,88]      0.76      0.04      0.76      0.70      0.83   5184.22      1.00\n",
      "alpha[0,89]      0.61      0.04      0.61      0.55      0.67   5004.49      1.00\n",
      "alpha[0,90]      0.70      0.04      0.70      0.63      0.76   4787.55      1.00\n",
      "alpha[0,91]      0.80      0.05      0.80      0.72      0.89   4862.42      1.00\n",
      "alpha[0,92]      0.83      0.05      0.83      0.76      0.91   3340.37      1.00\n",
      "alpha[0,93]      0.74      0.04      0.74      0.67      0.82   3683.28      1.00\n",
      "alpha[0,94]      0.82      0.04      0.82      0.75      0.89   5098.76      1.00\n",
      "alpha[0,95]      0.66      0.04      0.66      0.59      0.73   4582.56      1.00\n",
      "alpha[0,96]      0.72      0.04      0.72      0.65      0.79   3934.60      1.00\n",
      "alpha[0,97]      0.64      0.04      0.64      0.58      0.70   4814.23      1.00\n",
      "alpha[0,98]      0.64      0.04      0.64      0.59      0.70   4115.77      1.00\n",
      "alpha[0,99]      0.59      0.03      0.59      0.54      0.65   4164.52      1.00\n",
      " alpha[1,0]      0.91      0.07      0.91      0.80      1.02   4330.03      1.00\n",
      " alpha[1,1]      0.86      0.06      0.86      0.76      0.96   4571.88      1.00\n",
      " alpha[1,2]      0.91      0.06      0.91      0.82      1.02   4967.53      1.00\n",
      " alpha[1,3]      0.88      0.06      0.87      0.77      0.97   5485.98      1.00\n",
      " alpha[1,4]      0.93      0.06      0.92      0.83      1.03   4531.26      1.00\n",
      " alpha[1,5]      0.81      0.06      0.80      0.71      0.90   3745.80      1.00\n",
      " alpha[1,6]      0.93      0.07      0.93      0.81      1.02   4860.46      1.00\n",
      " alpha[1,7]      1.08      0.06      1.08      0.98      1.18   3622.12      1.00\n",
      " alpha[1,8]      0.85      0.06      0.85      0.75      0.95   4820.88      1.00\n",
      " alpha[1,9]      0.92      0.06      0.92      0.83      1.02   4382.36      1.00\n",
      "alpha[1,10]      0.70      0.05      0.70      0.63      0.77   4802.49      1.00\n",
      "alpha[1,11]      0.92      0.07      0.92      0.82      1.03   4308.62      1.00\n",
      "alpha[1,12]      0.98      0.06      0.98      0.86      1.07   4533.70      1.00\n",
      "alpha[1,13]      0.86      0.06      0.86      0.76      0.95   4517.28      1.00\n",
      "alpha[1,14]      0.91      0.06      0.91      0.81      1.02   3966.03      1.00\n",
      "alpha[1,15]      0.87      0.05      0.87      0.79      0.95   4142.68      1.00\n",
      "alpha[1,16]      0.99      0.06      0.99      0.89      1.10   3921.20      1.00\n",
      "alpha[1,17]      0.70      0.05      0.70      0.62      0.78   4400.80      1.00\n",
      "alpha[1,18]      0.77      0.05      0.77      0.69      0.85   3915.22      1.00\n",
      "alpha[1,19]      1.06      0.07      1.06      0.95      1.16   3904.06      1.00\n",
      "alpha[1,20]      0.87      0.06      0.87      0.77      0.97   4562.68      1.00\n",
      "alpha[1,21]      0.78      0.05      0.77      0.69      0.86   5039.44      1.00\n",
      "alpha[1,22]      0.73      0.05      0.73      0.66      0.82   3729.70      1.00\n",
      "alpha[1,23]      0.85      0.05      0.85      0.76      0.94   3766.36      1.00\n",
      "alpha[1,24]      0.80      0.05      0.80      0.71      0.87   3637.79      1.00\n",
      "alpha[1,25]      0.87      0.06      0.87      0.77      0.96   4342.73      1.00\n",
      "alpha[1,26]      1.03      0.06      1.03      0.91      1.13   3949.18      1.00\n",
      "alpha[1,27]      1.04      0.06      1.03      0.95      1.16   4229.71      1.00\n",
      "alpha[1,28]      0.67      0.04      0.67      0.61      0.75   4744.96      1.00\n",
      "alpha[1,29]      0.76      0.05      0.76      0.68      0.84   3676.91      1.00\n",
      "alpha[1,30]      0.69      0.04      0.69      0.62      0.76   4162.04      1.00\n",
      "alpha[1,31]      0.98      0.06      0.98      0.88      1.08   4846.89      1.00\n",
      "alpha[1,32]      0.73      0.04      0.73      0.66      0.80   4171.93      1.00\n",
      "alpha[1,33]      0.78      0.05      0.77      0.69      0.86   4170.10      1.00\n",
      "alpha[1,34]      0.80      0.05      0.79      0.72      0.87   4152.96      1.00\n",
      "alpha[1,35]      0.80      0.05      0.80      0.71      0.88   4357.67      1.00\n",
      "alpha[1,36]      0.78      0.05      0.78      0.70      0.85   4324.79      1.00\n",
      "alpha[1,37]      0.65      0.04      0.65      0.58      0.71   3763.68      1.00\n",
      "alpha[1,38]      1.02      0.07      1.02      0.91      1.13   4394.55      1.00\n",
      "alpha[1,39]      0.59      0.04      0.59      0.53      0.65   4981.85      1.00\n",
      "alpha[1,40]      0.85      0.05      0.85      0.77      0.93   3535.27      1.00\n",
      "alpha[1,41]      0.80      0.05      0.80      0.72      0.89   4353.49      1.00\n",
      "alpha[1,42]      0.84      0.05      0.84      0.75      0.92   3944.18      1.00\n",
      "alpha[1,43]      0.94      0.06      0.94      0.83      1.04   4476.51      1.00\n",
      "alpha[1,44]      0.94      0.06      0.94      0.85      1.03   5253.49      1.00\n",
      "alpha[1,45]      0.73      0.05      0.73      0.65      0.81   4019.07      1.00\n",
      "alpha[1,46]      0.92      0.05      0.92      0.82      1.00   3985.92      1.00\n",
      "alpha[1,47]      0.98      0.06      0.98      0.87      1.08   3680.41      1.00\n",
      "alpha[1,48]      1.08      0.06      1.08      0.98      1.19   3560.30      1.00\n",
      "alpha[1,49]      0.91      0.06      0.91      0.82      1.00   4786.06      1.00\n",
      "alpha[1,50]      0.79      0.05      0.79      0.71      0.87   4629.44      1.00\n",
      "alpha[1,51]      0.89      0.06      0.88      0.78      0.98   3999.17      1.00\n",
      "alpha[1,52]      0.61      0.04      0.61      0.55      0.68   3303.86      1.00\n",
      "alpha[1,53]      0.66      0.04      0.66      0.59      0.72   3823.21      1.00\n",
      "alpha[1,54]      0.73      0.05      0.73      0.65      0.80   4284.50      1.00\n",
      "alpha[1,55]      0.69      0.04      0.69      0.61      0.76   5245.13      1.00\n",
      "alpha[1,56]      0.62      0.04      0.61      0.55      0.68   4381.80      1.00\n",
      "alpha[1,57]      0.83      0.06      0.83      0.74      0.92   4416.11      1.00\n",
      "alpha[1,58]      0.63      0.04      0.63      0.56      0.69   4364.30      1.00\n",
      "alpha[1,59]      0.65      0.04      0.65      0.58      0.72   4605.90      1.00\n",
      "alpha[1,60]      0.66      0.04      0.66      0.59      0.73   4954.66      1.00\n",
      "alpha[1,61]      0.68      0.04      0.68      0.60      0.75   4542.90      1.00\n",
      "alpha[1,62]      0.81      0.05      0.81      0.73      0.90   3885.70      1.00\n",
      "alpha[1,63]      0.73      0.05      0.72      0.66      0.81   4266.42      1.00\n",
      "alpha[1,64]      0.75      0.04      0.75      0.68      0.83   3843.13      1.00\n",
      "alpha[1,65]      0.78      0.05      0.78      0.70      0.86   3869.60      1.00\n",
      "alpha[1,66]      0.71      0.05      0.71      0.63      0.79   3234.82      1.00\n",
      "alpha[1,67]      0.78      0.05      0.78      0.71      0.87   5015.00      1.00\n",
      "alpha[1,68]      0.79      0.05      0.79      0.71      0.86   4065.76      1.00\n",
      "alpha[1,69]      0.66      0.05      0.66      0.58      0.74   6112.52      1.00\n",
      "alpha[1,70]      0.66      0.04      0.66      0.60      0.72   3369.89      1.00\n",
      "alpha[1,71]      0.73      0.05      0.72      0.65      0.80   4796.21      1.00\n",
      "alpha[1,72]      0.88      0.06      0.87      0.79      0.97   3696.01      1.00\n",
      "alpha[1,73]      0.90      0.06      0.90      0.81      1.00   4424.22      1.00\n",
      "alpha[1,74]      0.83      0.05      0.83      0.75      0.91   4839.66      1.00\n",
      "alpha[1,75]      0.69      0.04      0.68      0.61      0.75   4302.16      1.00\n",
      "alpha[1,76]      0.60      0.04      0.60      0.53      0.66   4197.89      1.00\n",
      "alpha[1,77]      0.73      0.04      0.73      0.67      0.81   4584.57      1.00\n",
      "alpha[1,78]      0.62      0.04      0.62      0.56      0.69   4072.44      1.00\n",
      "alpha[1,79]      0.70      0.04      0.70      0.64      0.77   4718.46      1.00\n",
      "alpha[1,80]      0.66      0.05      0.66      0.58      0.74   4526.40      1.00\n",
      "alpha[1,81]      0.74      0.04      0.73      0.67      0.81   4255.46      1.00\n",
      "alpha[1,82]      0.88      0.05      0.88      0.79      0.96   3994.22      1.00\n",
      "alpha[1,83]      0.64      0.05      0.64      0.56      0.72   4441.41      1.00\n",
      "alpha[1,84]      0.66      0.05      0.65      0.58      0.73   3513.24      1.00\n",
      "alpha[1,85]      0.69      0.04      0.69      0.62      0.76   3734.78      1.00\n",
      "alpha[1,86]      0.74      0.05      0.74      0.66      0.82   3989.74      1.00\n",
      "alpha[1,87]      0.57      0.04      0.57      0.52      0.63   4341.79      1.00\n",
      "alpha[1,88]      0.64      0.04      0.64      0.58      0.70   4822.31      1.00\n",
      "alpha[1,89]      0.62      0.04      0.62      0.57      0.69   3829.50      1.00\n",
      "alpha[1,90]      0.59      0.04      0.59      0.53      0.65   4527.78      1.00\n",
      "alpha[1,91]      0.73      0.04      0.73      0.66      0.81   4028.96      1.00\n",
      "alpha[1,92]      0.78      0.05      0.78      0.70      0.85   4783.84      1.00\n",
      "alpha[1,93]      0.60      0.04      0.60      0.54      0.68   4480.85      1.00\n",
      "alpha[1,94]      0.65      0.04      0.65      0.59      0.72   3806.40      1.00\n",
      "alpha[1,95]      0.72      0.05      0.72      0.64      0.80   4131.19      1.00\n",
      "alpha[1,96]      0.78      0.04      0.78      0.71      0.86   4814.63      1.00\n",
      "alpha[1,97]      0.77      0.05      0.77      0.68      0.84   4513.60      1.00\n",
      "alpha[1,98]      0.52      0.03      0.52      0.47      0.56   5065.70      1.00\n",
      "alpha[1,99]      0.55      0.03      0.55      0.49      0.59   4445.11      1.00\n",
      "  beta[0,0]      0.27      0.04      0.27      0.19      0.33   4684.85      1.00\n",
      "  beta[0,1]      0.53      0.07      0.53      0.42      0.64   4516.42      1.00\n",
      "  beta[0,2]      0.59      0.06      0.59      0.48      0.69   3450.41      1.00\n",
      "  beta[0,3]      0.65      0.08      0.64      0.51      0.77   2831.12      1.00\n",
      "  beta[0,4]      0.75      0.09      0.75      0.59      0.91   4114.38      1.00\n",
      "  beta[0,5]      0.16      0.02      0.16      0.12      0.20   4254.55      1.00\n",
      "  beta[0,6]      0.54      0.07      0.54      0.44      0.65   4023.37      1.00\n",
      "  beta[0,7]      1.08      0.11      1.08      0.90      1.27   3605.75      1.00\n",
      "  beta[0,8]      0.13      0.02      0.13      0.10      0.16   4741.54      1.00\n",
      "  beta[0,9]      1.20      0.12      1.20      1.01      1.40   4263.85      1.00\n",
      " beta[0,10]      1.11      0.13      1.10      0.91      1.33   3165.01      1.00\n",
      " beta[0,11]      0.95      0.09      0.95      0.81      1.11   3790.82      1.00\n",
      " beta[0,12]      0.74      0.08      0.74      0.62      0.87   4760.79      1.00\n",
      " beta[0,13]      0.14      0.02      0.14      0.11      0.17   4821.22      1.00\n",
      " beta[0,14]      0.30      0.03      0.30      0.24      0.35   5357.55      1.00\n",
      " beta[0,15]      0.44      0.05      0.44      0.36      0.52   3257.73      1.00\n",
      " beta[0,16]      0.52      0.06      0.52      0.43      0.61   4632.72      1.00\n",
      " beta[0,17]      1.03      0.12      1.04      0.85      1.24   4076.83      1.00\n",
      " beta[0,18]      0.85      0.09      0.84      0.70      1.01   4138.75      1.00\n",
      " beta[0,19]      0.66      0.07      0.66      0.55      0.78   4205.72      1.00\n",
      " beta[0,20]      0.14      0.01      0.14      0.11      0.16   4184.22      1.00\n",
      " beta[0,21]      0.51      0.05      0.50      0.43      0.59   5157.69      1.00\n",
      " beta[0,22]      0.76      0.08      0.76      0.64      0.90   4812.28      1.00\n",
      " beta[0,23]      0.20      0.02      0.20      0.17      0.23   4058.81      1.00\n",
      " beta[0,24]      0.62      0.06      0.62      0.51      0.72   5430.30      1.00\n",
      " beta[0,25]      0.40      0.04      0.40      0.34      0.46   4608.24      1.00\n",
      " beta[0,26]      0.62      0.06      0.62      0.53      0.72   4363.09      1.00\n",
      " beta[0,27]      0.69      0.07      0.69      0.58      0.81   5533.94      1.00\n",
      " beta[0,28]      0.53      0.05      0.52      0.45      0.61   4231.58      1.00\n",
      " beta[0,29]      0.34      0.03      0.34      0.28      0.39   4070.11      1.00\n",
      " beta[0,30]      0.25      0.02      0.24      0.21      0.28   4554.13      1.00\n",
      " beta[0,31]      0.49      0.04      0.49      0.42      0.56   5072.90      1.00\n",
      " beta[0,32]      0.31      0.03      0.30      0.26      0.35   3770.53      1.00\n",
      " beta[0,33]      0.39      0.04      0.39      0.33      0.45   3861.09      1.00\n",
      " beta[0,34]      0.30      0.02      0.30      0.26      0.34   3271.83      1.00\n",
      " beta[0,35]      0.41      0.03      0.41      0.36      0.47   3857.58      1.00\n",
      " beta[0,36]      0.38      0.03      0.38      0.34      0.43   4409.89      1.00\n",
      " beta[0,37]      0.44      0.04      0.44      0.38      0.50   4562.80      1.00\n",
      " beta[0,38]      0.44      0.04      0.44      0.37      0.50   4260.04      1.00\n",
      " beta[0,39]      0.26      0.03      0.26      0.21      0.30   4521.48      1.00\n",
      " beta[0,40]      0.20      0.02      0.20      0.17      0.22   5346.86      1.00\n",
      " beta[0,41]      0.84      0.07      0.84      0.73      0.95   3594.71      1.00\n",
      " beta[0,42]      0.50      0.05      0.50      0.42      0.57   4747.05      1.00\n",
      " beta[0,43]      0.46      0.04      0.46      0.40      0.52   3154.63      1.00\n",
      " beta[0,44]      0.37      0.03      0.37      0.33      0.42   4291.58      1.00\n",
      " beta[0,45]      0.56      0.07      0.56      0.45      0.66   4223.16      1.00\n",
      " beta[0,46]      0.36      0.03      0.36      0.32      0.40   3958.26      1.00\n",
      " beta[0,47]      0.82      0.07      0.82      0.70      0.94   4340.96      1.00\n",
      " beta[0,48]      0.76      0.07      0.76      0.64      0.88   4379.22      1.00\n",
      " beta[0,49]      0.74      0.06      0.74      0.63      0.83   4503.64      1.00\n",
      " beta[0,50]      0.34      0.03      0.34      0.29      0.39   3739.45      1.00\n",
      " beta[0,51]      0.36      0.03      0.35      0.31      0.40   4746.17      1.00\n",
      " beta[0,52]      0.15      0.01      0.15      0.13      0.17   3835.52      1.00\n",
      " beta[0,53]      0.20      0.02      0.20      0.17      0.23   3973.78      1.00\n",
      " beta[0,54]      0.21      0.02      0.21      0.18      0.25   4544.53      1.00\n",
      " beta[0,55]      0.30      0.03      0.30      0.25      0.36   4244.26      1.00\n",
      " beta[0,56]      0.14      0.01      0.14      0.12      0.16   5503.78      1.00\n",
      " beta[0,57]      0.24      0.02      0.24      0.21      0.27   3577.41      1.00\n",
      " beta[0,58]      0.11      0.01      0.11      0.09      0.13   4909.63      1.00\n",
      " beta[0,59]      0.17      0.02      0.17      0.14      0.19   4330.50      1.00\n",
      " beta[0,60]      0.20      0.02      0.20      0.17      0.23   4634.13      1.00\n",
      " beta[0,61]      0.24      0.03      0.24      0.20      0.28   4549.84      1.00\n",
      " beta[0,62]      0.32      0.02      0.32      0.28      0.36   3417.78      1.00\n",
      " beta[0,63]      0.20      0.02      0.20      0.17      0.23   4739.64      1.00\n",
      " beta[0,64]      0.57      0.05      0.57      0.49      0.65   5756.02      1.00\n",
      " beta[0,65]      0.26      0.02      0.26      0.22      0.30   3948.69      1.00\n",
      " beta[0,66]      0.24      0.02      0.24      0.21      0.28   3678.55      1.00\n",
      " beta[0,67]      0.58      0.04      0.58      0.51      0.65   4422.08      1.00\n",
      " beta[0,68]      0.73      0.07      0.73      0.62      0.86   4281.40      1.00\n",
      " beta[0,69]      0.21      0.02      0.21      0.19      0.24   4264.45      1.00\n",
      " beta[0,70]      0.32      0.03      0.32      0.27      0.37   4061.75      1.00\n",
      " beta[0,71]      0.44      0.04      0.44      0.37      0.51   4572.22      1.00\n",
      " beta[0,72]      0.67      0.06      0.67      0.57      0.78   4405.43      1.00\n",
      " beta[0,73]      0.63      0.07      0.63      0.51      0.74   4454.85      1.00\n",
      " beta[0,74]      0.66      0.05      0.66      0.58      0.73   4153.04      1.00\n",
      " beta[0,75]      0.41      0.04      0.41      0.35      0.49   4185.21      1.00\n",
      " beta[0,76]      0.32      0.03      0.32      0.27      0.38   4429.35      1.00\n",
      " beta[0,77]      0.53      0.04      0.52      0.46      0.60   4451.80      1.00\n",
      " beta[0,78]      0.31      0.04      0.31      0.25      0.37   4528.19      1.00\n",
      " beta[0,79]      0.34      0.03      0.34      0.28      0.39   3744.32      1.00\n",
      " beta[0,80]      0.19      0.02      0.19      0.17      0.22   4714.18      1.00\n",
      " beta[0,81]      0.17      0.02      0.16      0.14      0.19   4003.54      1.00\n",
      " beta[0,82]      0.23      0.02      0.23      0.20      0.26   3879.01      1.00\n",
      " beta[0,83]      0.18      0.02      0.18      0.16      0.21   4640.08      1.00\n",
      " beta[0,84]      0.15      0.01      0.15      0.13      0.18   4321.73      1.00\n",
      " beta[0,85]      0.20      0.02      0.20      0.17      0.23   4143.32      1.00\n",
      " beta[0,86]      0.27      0.02      0.27      0.24      0.30   4272.15      1.00\n",
      " beta[0,87]      0.11      0.01      0.11      0.09      0.13   3924.33      1.00\n",
      " beta[0,88]      0.30      0.02      0.30      0.26      0.34   5006.48      1.00\n",
      " beta[0,89]      0.08      0.01      0.08      0.07      0.09   5241.65      1.00\n",
      " beta[0,90]      0.09      0.01      0.09      0.08      0.10   3885.88      1.00\n",
      " beta[0,91]      0.27      0.03      0.27      0.23      0.31   5067.10      1.00\n",
      " beta[0,92]      0.13      0.01      0.13      0.12      0.15   4190.10      1.00\n",
      " beta[0,93]      0.11      0.01      0.11      0.09      0.13   3487.40      1.00\n",
      " beta[0,94]      0.21      0.02      0.21      0.19      0.24   4500.33      1.00\n",
      " beta[0,95]      0.13      0.01      0.13      0.11      0.15   4520.41      1.00\n",
      " beta[0,96]      0.21      0.02      0.21      0.19      0.24   3627.98      1.00\n",
      " beta[0,97]      0.16      0.01      0.16      0.13      0.18   4107.11      1.00\n",
      " beta[0,98]      0.16      0.01      0.16      0.14      0.18   4564.76      1.00\n",
      " beta[0,99]      0.15      0.01      0.15      0.13      0.17   4137.63      1.00\n",
      "  beta[1,0]      0.55      0.06      0.55      0.46      0.64   4056.48      1.00\n",
      "  beta[1,1]      0.52      0.05      0.52      0.44      0.60   4340.27      1.00\n",
      "  beta[1,2]      0.58      0.05      0.58      0.50      0.67   4460.83      1.00\n",
      "  beta[1,3]      0.60      0.06      0.60      0.51      0.69   4636.97      1.00\n",
      "  beta[1,4]      0.67      0.06      0.67      0.57      0.76   4327.25      1.00\n",
      "  beta[1,5]      0.39      0.04      0.39      0.32      0.45   4900.19      1.00\n",
      "  beta[1,6]      0.61      0.06      0.61      0.51      0.71   4451.85      1.00\n",
      "  beta[1,7]      1.63      0.13      1.62      1.42      1.84   5086.04      1.00\n",
      "  beta[1,8]      0.41      0.04      0.40      0.34      0.47   4534.84      1.00\n",
      "  beta[1,9]      0.92      0.08      0.92      0.79      1.06   4328.53      1.00\n",
      " beta[1,10]      0.38      0.04      0.38      0.33      0.44   5025.36      1.00\n",
      " beta[1,11]      0.78      0.08      0.78      0.65      0.90   3978.68      1.00\n",
      " beta[1,12]      0.79      0.07      0.79      0.67      0.89   4193.65      1.00\n",
      " beta[1,13]      0.35      0.03      0.35      0.30      0.41   5200.21      1.00\n",
      " beta[1,14]      0.48      0.05      0.48      0.41      0.56   4450.71      1.00\n",
      " beta[1,15]      0.40      0.03      0.40      0.35      0.46   4646.27      1.00\n",
      " beta[1,16]      0.66      0.06      0.66      0.56      0.75   4545.34      1.00\n",
      " beta[1,17]      0.36      0.04      0.36      0.30      0.42   4747.87      1.00\n",
      " beta[1,18]      0.39      0.04      0.39      0.33      0.44   4715.19      1.00\n",
      " beta[1,19]      0.78      0.07      0.78      0.68      0.89   4129.47      1.00\n",
      " beta[1,20]      0.33      0.03      0.33      0.28      0.38   4287.09      1.00\n",
      " beta[1,21]      0.41      0.04      0.41      0.35      0.47   4713.47      1.00\n",
      " beta[1,22]      0.37      0.03      0.37      0.31      0.43   3680.58      1.00\n",
      " beta[1,23]      0.34      0.03      0.34      0.29      0.40   4576.60      1.00\n",
      " beta[1,24]      0.37      0.03      0.37      0.32      0.42   3911.01      1.00\n",
      " beta[1,25]      0.44      0.04      0.44      0.37      0.50   4963.45      1.00\n",
      " beta[1,26]      0.65      0.06      0.65      0.56      0.74   3619.06      1.00\n",
      " beta[1,27]      0.79      0.07      0.79      0.68      0.89   4603.19      1.00\n",
      " beta[1,28]      0.26      0.02      0.26      0.22      0.29   4539.95      1.00\n",
      " beta[1,29]      0.32      0.03      0.32      0.27      0.37   4608.08      1.00\n",
      " beta[1,30]      0.23      0.02      0.23      0.20      0.27   4784.08      1.00\n",
      " beta[1,31]      0.64      0.06      0.64      0.55      0.73   3651.55      1.00\n",
      " beta[1,32]      0.27      0.02      0.27      0.24      0.32   3442.65      1.00\n",
      " beta[1,33]      0.30      0.03      0.29      0.25      0.34   5020.21      1.00\n",
      " beta[1,34]      0.32      0.03      0.32      0.28      0.36   4449.67      1.00\n",
      " beta[1,35]      0.44      0.04      0.44      0.37      0.51   5440.81      1.00\n",
      " beta[1,36]      0.32      0.03      0.32      0.28      0.36   4670.20      1.00\n",
      " beta[1,37]      0.21      0.02      0.21      0.18      0.24   4178.57      1.00\n",
      " beta[1,38]      0.79      0.07      0.79      0.69      0.92   4642.96      1.00\n",
      " beta[1,39]      0.14      0.01      0.14      0.12      0.16   4922.96      1.00\n",
      " beta[1,40]      0.32      0.03      0.32      0.27      0.36   3662.67      1.00\n",
      " beta[1,41]      0.66      0.06      0.65      0.56      0.75   4534.12      1.00\n",
      " beta[1,42]      0.56      0.05      0.56      0.48      0.65   4181.48      1.00\n",
      " beta[1,43]      0.62      0.06      0.62      0.53      0.71   4976.32      1.00\n",
      " beta[1,44]      0.58      0.05      0.58      0.51      0.67   4954.48      1.00\n",
      " beta[1,45]      0.29      0.03      0.29      0.25      0.34   4315.88      1.00\n",
      " beta[1,46]      0.49      0.04      0.48      0.42      0.55   4386.63      1.00\n",
      " beta[1,47]      1.01      0.09      1.00      0.87      1.15   4023.58      1.00\n",
      " beta[1,48]      1.39      0.11      1.39      1.21      1.56   3955.16      1.00\n",
      " beta[1,49]      0.54      0.04      0.54      0.46      0.61   3814.34      1.00\n",
      " beta[1,50]      0.23      0.02      0.23      0.20      0.27   4854.36      1.00\n",
      " beta[1,51]      0.73      0.07      0.73      0.61      0.84   3648.84      1.00\n",
      " beta[1,52]      0.10      0.01      0.10      0.09      0.12   3707.98      1.00\n",
      " beta[1,53]      0.13      0.01      0.13      0.11      0.15   4733.07      1.00\n",
      " beta[1,54]      0.13      0.01      0.13      0.11      0.15   3505.71      1.00\n",
      " beta[1,55]      0.12      0.01      0.12      0.10      0.13   4127.92      1.00\n",
      " beta[1,56]      0.08      0.01      0.08      0.07      0.09   4233.18      1.00\n",
      " beta[1,57]      0.52      0.05      0.52      0.44      0.60   4567.72      1.00\n",
      " beta[1,58]      0.09      0.01      0.09      0.07      0.10   3996.54      1.00\n",
      " beta[1,59]      0.10      0.01      0.10      0.08      0.11   4303.97      1.00\n",
      " beta[1,60]      0.11      0.01      0.11      0.09      0.13   3682.46      1.00\n",
      " beta[1,61]      0.11      0.01      0.11      0.10      0.13   3743.83      1.00\n",
      " beta[1,62]      0.53      0.05      0.53      0.45      0.62   4384.98      1.00\n",
      " beta[1,63]      0.43      0.04      0.43      0.36      0.50   4271.18      1.00\n",
      " beta[1,64]      0.31      0.03      0.31      0.27      0.35   4113.95      1.00\n",
      " beta[1,65]      0.15      0.01      0.15      0.13      0.18   4570.51      1.00\n",
      " beta[1,66]      0.39      0.04      0.39      0.32      0.45   4000.54      1.00\n",
      " beta[1,67]      0.43      0.03      0.43      0.37      0.49   4341.99      1.00\n",
      " beta[1,68]      0.26      0.02      0.26      0.22      0.30   4184.68      1.00\n",
      " beta[1,69]      0.32      0.03      0.32      0.26      0.37   4741.95      1.00\n",
      " beta[1,70]      0.13      0.01      0.13      0.11      0.15   3793.84      1.00\n",
      " beta[1,71]      0.19      0.02      0.19      0.16      0.22   3888.72      1.00\n",
      " beta[1,72]      0.34      0.03      0.34      0.29      0.39   3408.79      1.00\n",
      " beta[1,73]      0.59      0.05      0.59      0.51      0.69   3853.62      1.00\n",
      " beta[1,74]      0.54      0.04      0.54      0.48      0.62   5395.60      1.00\n",
      " beta[1,75]      0.16      0.01      0.16      0.14      0.19   4574.67      1.00\n",
      " beta[1,76]      0.12      0.01      0.12      0.10      0.14   4705.95      1.00\n",
      " beta[1,77]      0.25      0.02      0.25      0.22      0.29   4184.45      1.00\n",
      " beta[1,78]      0.16      0.02      0.16      0.13      0.19   4149.34      1.00\n",
      " beta[1,79]      0.25      0.02      0.25      0.21      0.28   4968.34      1.00\n",
      " beta[1,80]      0.25      0.03      0.25      0.20      0.30   4217.49      1.00\n",
      " beta[1,81]      0.13      0.01      0.13      0.11      0.15   4917.71      1.00\n",
      " beta[1,82]      0.28      0.02      0.28      0.25      0.32   4092.01      1.00\n",
      " beta[1,83]      0.24      0.03      0.24      0.20      0.28   4846.22      1.00\n",
      " beta[1,84]      0.22      0.02      0.22      0.18      0.26   4978.37      1.00\n",
      " beta[1,85]      0.11      0.01      0.11      0.09      0.13   3698.91      1.00\n",
      " beta[1,86]      0.36      0.03      0.36      0.31      0.42   4779.32      1.00\n",
      " beta[1,87]      0.06      0.01      0.06      0.05      0.07   4630.42      1.00\n",
      " beta[1,88]      0.16      0.01      0.16      0.14      0.18   3918.32      1.00\n",
      " beta[1,89]      0.07      0.01      0.07      0.06      0.08   5027.40      1.00\n",
      " beta[1,90]      0.13      0.01      0.13      0.11      0.16   4075.88      1.00\n",
      " beta[1,91]      0.15      0.01      0.15      0.13      0.17   4646.85      1.00\n",
      " beta[1,92]      0.38      0.03      0.38      0.33      0.44   4284.00      1.00\n",
      " beta[1,93]      0.17      0.02      0.17      0.14      0.20   5231.57      1.00\n",
      " beta[1,94]      0.16      0.01      0.16      0.14      0.18   4221.11      1.00\n",
      " beta[1,95]      0.32      0.03      0.32      0.27      0.37   4471.40      1.00\n",
      " beta[1,96]      0.31      0.02      0.31      0.27      0.35   5007.54      1.00\n",
      " beta[1,97]      0.34      0.03      0.34      0.29      0.39   4414.40      1.00\n",
      " beta[1,98]      0.08      0.01      0.08      0.07      0.09   4448.21      1.00\n",
      " beta[1,99]      0.13      0.01      0.13      0.11      0.15   4260.29      1.00\n",
      "    betavar      0.43      0.03      0.43      0.39      0.47   2739.97      1.00\n",
      "     p[0,0]      0.21      0.02      0.21      0.18      0.23   3867.52      1.00\n",
      "     p[0,1]      0.36      0.02      0.36      0.33      0.39   3090.52      1.00\n",
      "     p[0,2]      0.42      0.02      0.42      0.38      0.45   3560.75      1.00\n",
      "     p[0,3]      0.34      0.02      0.34      0.31      0.37   3718.78      1.00\n",
      "     p[0,4]      0.33      0.02      0.33      0.30      0.36   3534.63      1.00\n",
      "     p[0,5]      0.24      0.02      0.24      0.21      0.27   3380.65      1.00\n",
      "     p[0,6]      0.39      0.02      0.39      0.35      0.42   5842.33      1.00\n",
      "     p[0,7]      0.42      0.02      0.42      0.39      0.46   3858.05      1.00\n",
      "     p[0,8]      0.29      0.02      0.29      0.26      0.32   3487.49      1.00\n",
      "     p[0,9]      0.41      0.02      0.41      0.38      0.45   4606.65      1.00\n",
      "    p[0,10]      0.31      0.02      0.31      0.27      0.34   3971.78      1.00\n",
      "    p[0,11]      0.45      0.02      0.45      0.42      0.48   3733.20      1.00\n",
      "    p[0,12]      0.41      0.02      0.41      0.37      0.44   4382.31      1.00\n",
      "    p[0,13]      0.41      0.02      0.41      0.37      0.44   3154.36      1.00\n",
      "    p[0,14]      0.44      0.02      0.44      0.40      0.47   4914.27      1.00\n",
      "    p[0,15]      0.44      0.02      0.44      0.41      0.48   5332.40      1.00\n",
      "    p[0,16]      0.44      0.02      0.44      0.40      0.47   3406.69      1.00\n",
      "    p[0,17]      0.32      0.02      0.32      0.29      0.36   3400.27      1.00\n",
      "    p[0,18]      0.38      0.02      0.38      0.35      0.42   2655.82      1.00\n",
      "    p[0,19]      0.46      0.02      0.46      0.43      0.49   3788.15      1.00\n",
      "    p[0,20]      0.50      0.02      0.50      0.47      0.53   4320.29      1.00\n",
      "    p[0,21]      0.42      0.02      0.42      0.39      0.46   4489.71      1.00\n",
      "    p[0,22]      0.44      0.02      0.44      0.41      0.47   3653.67      1.00\n",
      "    p[0,23]      0.56      0.02      0.56      0.53      0.59   4053.20      1.00\n",
      "    p[0,24]      0.48      0.02      0.48      0.44      0.51   4656.45      1.00\n",
      "    p[0,25]      0.54      0.02      0.54      0.51      0.58   3850.25      1.00\n",
      "    p[0,26]      0.57      0.02      0.57      0.53      0.60   4262.85      1.00\n",
      "    p[0,27]      0.49      0.02      0.49      0.46      0.53   3425.54      1.00\n",
      "    p[0,28]      0.49      0.02      0.49      0.45      0.52   3671.35      1.00\n",
      "    p[0,29]      0.56      0.02      0.56      0.53      0.60   5035.94      1.00\n",
      "    p[0,30]      0.61      0.02      0.61      0.58      0.64   4705.45      1.00\n",
      "    p[0,31]      0.62      0.02      0.62      0.59      0.65   2749.38      1.00\n",
      "    p[0,32]      0.66      0.02      0.66      0.62      0.69   4391.24      1.00\n",
      "    p[0,33]      0.53      0.02      0.53      0.49      0.56   3226.11      1.00\n",
      "    p[0,34]      0.68      0.02      0.68      0.64      0.71   3445.94      1.00\n",
      "    p[0,35]      0.69      0.02      0.69      0.66      0.72   4023.16      1.00\n",
      "    p[0,36]      0.72      0.02      0.72      0.69      0.75   4171.12      1.00\n",
      "    p[0,37]      0.65      0.02      0.65      0.61      0.68   3212.76      1.00\n",
      "    p[0,38]      0.60      0.02      0.60      0.57      0.64   3742.32      1.00\n",
      "    p[0,39]      0.44      0.02      0.44      0.41      0.47   2828.23      1.00\n",
      "    p[0,40]      0.65      0.02      0.65      0.62      0.69   4029.48      1.00\n",
      "    p[0,41]      0.67      0.02      0.67      0.64      0.70   4240.50      1.00\n",
      "    p[0,42]      0.60      0.02      0.60      0.57      0.64   3701.14      1.00\n",
      "    p[0,43]      0.68      0.02      0.68      0.65      0.71   4410.30      1.00\n",
      "    p[0,44]      0.79      0.02      0.79      0.76      0.82   3940.90      1.00\n",
      "    p[0,45]      0.35      0.02      0.35      0.32      0.39   4075.64      1.00\n",
      "    p[0,46]      0.82      0.02      0.82      0.79      0.85   4260.56      1.00\n",
      "    p[0,47]      0.53      0.02      0.53      0.49      0.56   4319.50      1.00\n",
      "    p[0,48]      0.57      0.02      0.57      0.54      0.60   3545.51      1.00\n",
      "    p[0,49]      0.61      0.02      0.61      0.57      0.64   5218.21      1.00\n",
      "    p[0,50]      0.58      0.02      0.58      0.55      0.61   2851.27      1.00\n",
      "    p[0,51]      0.76      0.02      0.76      0.73      0.79   4008.17      1.00\n",
      "    p[0,52]      0.53      0.02      0.53      0.49      0.56   3692.57      1.00\n",
      "    p[0,53]      0.59      0.02      0.59      0.56      0.62   3123.04      1.00\n",
      "    p[0,54]      0.51      0.02      0.51      0.48      0.54   3513.11      1.00\n",
      "    p[0,55]      0.45      0.02      0.45      0.42      0.49   3044.64      1.00\n",
      "    p[0,56]      0.48      0.02      0.48      0.44      0.51   4693.85      1.00\n",
      "    p[0,57]      0.75      0.02      0.75      0.72      0.79   5024.50      1.00\n",
      "    p[0,58]      0.55      0.02      0.55      0.52      0.58   3926.51      1.00\n",
      "    p[0,59]      0.58      0.02      0.58      0.54      0.61   2759.67      1.00\n",
      "    p[0,60]      0.49      0.02      0.49      0.46      0.53   4958.77      1.00\n",
      "    p[0,61]      0.44      0.02      0.44      0.41      0.48   4350.22      1.00\n",
      "    p[0,62]      0.77      0.02      0.77      0.74      0.80   3850.28      1.00\n",
      "    p[0,63]      0.71      0.02      0.71      0.68      0.74   4560.89      1.00\n",
      "    p[0,64]      0.67      0.02      0.67      0.64      0.70   5243.51      1.00\n",
      "    p[0,65]      0.53      0.02      0.53      0.50      0.56   3320.99      1.00\n",
      "    p[0,66]      0.74      0.02      0.75      0.71      0.77   3572.66      1.00\n",
      "    p[0,67]      0.84      0.01      0.84      0.81      0.86   3176.65      1.00\n",
      "    p[0,68]      0.47      0.02      0.47      0.44      0.51   3634.90      1.00\n",
      "    p[0,69]      0.73      0.02      0.73      0.71      0.76   3698.94      1.00\n",
      "    p[0,70]      0.53      0.02      0.53      0.49      0.56   3754.27      1.00\n",
      "    p[0,71]      0.52      0.02      0.52      0.49      0.56   4027.43      1.00\n",
      "    p[0,72]      0.47      0.02      0.47      0.44      0.51   3397.01      1.00\n",
      "    p[0,73]      0.36      0.02      0.36      0.33      0.39   3255.64      1.00\n",
      "    p[0,74]      0.82      0.02      0.82      0.79      0.84   3346.94      1.00\n",
      "    p[0,75]      0.49      0.02      0.49      0.45      0.52   3892.44      1.00\n",
      "    p[0,76]      0.43      0.02      0.43      0.40      0.46   3711.51      1.00\n",
      "    p[0,77]      0.65      0.02      0.65      0.62      0.68   3736.21      1.00\n",
      "    p[0,78]      0.39      0.02      0.39      0.36      0.42   3309.90      1.00\n",
      "    p[0,79]      0.56      0.02      0.56      0.53      0.60   3582.27      1.00\n",
      "    p[0,80]      0.71      0.02      0.71      0.68      0.74   3470.41      1.00\n",
      "    p[0,81]      0.58      0.02      0.58      0.55      0.62   4245.19      1.00\n",
      "    p[0,82]      0.69      0.02      0.69      0.66      0.72   3661.49      1.00\n",
      "    p[0,83]      0.72      0.02      0.72      0.69      0.75   4350.05      1.00\n",
      "    p[0,84]      0.67      0.02      0.67      0.64      0.70   4197.71      1.00\n",
      "    p[0,85]      0.54      0.02      0.54      0.51      0.57   4356.65      1.00\n",
      "    p[0,86]      0.78      0.02      0.78      0.75      0.81   3932.93      1.00\n",
      "    p[0,87]      0.52      0.02      0.52      0.49      0.55   4151.85      1.00\n",
      "    p[0,88]      0.82      0.02      0.82      0.80      0.85   5083.10      1.00\n",
      "    p[0,89]      0.67      0.02      0.67      0.64      0.70   3341.14      1.00\n",
      "    p[0,90]      0.73      0.02      0.73      0.69      0.75   3835.29      1.00\n",
      "    p[0,91]      0.56      0.02      0.56      0.53      0.60   3239.93      1.00\n",
      "    p[0,92]      0.79      0.02      0.79      0.76      0.82   3782.71      1.00\n",
      "    p[0,93]      0.67      0.02      0.67      0.63      0.70   4775.73      1.00\n",
      "    p[0,94]      0.87      0.01      0.87      0.85      0.89   4764.85      1.00\n",
      "    p[0,95]      0.55      0.02      0.55      0.51      0.58   5737.95      1.00\n",
      "    p[0,96]      0.75      0.02      0.75      0.71      0.77   3594.02      1.00\n",
      "    p[0,97]      0.61      0.02      0.61      0.57      0.64   3683.88      1.00\n",
      "    p[0,98]      0.82      0.02      0.82      0.79      0.84   5405.05      1.00\n",
      "    p[0,99]      0.69      0.02      0.69      0.66      0.72   3289.45      1.00\n",
      "     p[1,0]      0.39      0.02      0.39      0.36      0.43   3828.29      1.00\n",
      "     p[1,1]      0.44      0.02      0.44      0.41      0.48   4245.84      1.00\n",
      "     p[1,2]      0.45      0.02      0.45      0.42      0.48   4853.02      1.00\n",
      "     p[1,3]      0.47      0.02      0.47      0.44      0.50   3214.05      1.00\n",
      "     p[1,4]      0.46      0.02      0.46      0.43      0.49   3779.26      1.00\n",
      "     p[1,5]      0.42      0.02      0.42      0.38      0.45   3993.38      1.00\n",
      "     p[1,6]      0.44      0.02      0.44      0.40      0.47   5393.55      1.00\n",
      "     p[1,7]      0.61      0.02      0.61      0.58      0.64   5426.93      1.00\n",
      "     p[1,8]      0.42      0.02      0.42      0.38      0.45   2937.61      1.00\n",
      "     p[1,9]      0.54      0.02      0.54      0.51      0.57   3237.14      1.00\n",
      "    p[1,10]      0.54      0.02      0.54      0.50      0.57   3445.74      1.00\n",
      "    p[1,11]      0.44      0.02      0.44      0.41      0.47   4156.50      1.00\n",
      "    p[1,12]      0.49      0.02      0.49      0.46      0.53   3954.89      1.00\n",
      "    p[1,13]      0.48      0.02      0.48      0.45      0.52   3724.33      1.00\n",
      "    p[1,14]      0.48      0.02      0.48      0.45      0.52   3422.83      1.00\n",
      "    p[1,15]      0.61      0.02      0.61      0.58      0.65   3898.62      1.00\n",
      "    p[1,16]      0.51      0.02      0.51      0.48      0.55   3717.43      1.00\n",
      "    p[1,17]      0.53      0.02      0.53      0.49      0.56   2870.87      1.00\n",
      "    p[1,18]      0.56      0.02      0.56      0.53      0.60   3423.79      1.00\n",
      "    p[1,19]      0.51      0.02      0.51      0.47      0.54   5813.13      1.00\n",
      "    p[1,20]      0.52      0.02      0.52      0.49      0.56   4971.71      1.00\n",
      "    p[1,21]      0.55      0.02      0.55      0.51      0.58   3989.65      1.00\n",
      "    p[1,22]      0.60      0.02      0.60      0.56      0.63   4222.90      1.00\n",
      "    p[1,23]      0.55      0.02      0.55      0.51      0.58   3438.77      1.00\n",
      "    p[1,24]      0.62      0.02      0.62      0.59      0.66   3742.47      1.00\n",
      "    p[1,25]      0.59      0.02      0.59      0.56      0.63   3656.51      1.00\n",
      "    p[1,26]      0.58      0.02      0.58      0.54      0.61   3153.37      1.00\n",
      "    p[1,27]      0.51      0.02      0.51      0.48      0.55   4672.97      1.00\n",
      "    p[1,28]      0.62      0.02      0.62      0.59      0.66   3673.46      1.00\n",
      "    p[1,29]      0.58      0.02      0.58      0.55      0.62   6507.91      1.00\n",
      "    p[1,30]      0.62      0.02      0.62      0.59      0.65   2966.04      1.00\n",
      "    p[1,31]      0.55      0.02      0.55      0.52      0.58   5941.51      1.00\n",
      "    p[1,32]      0.66      0.02      0.66      0.63      0.69   3824.48      1.00\n",
      "    p[1,33]      0.60      0.02      0.60      0.56      0.63   4397.11      1.00\n",
      "    p[1,34]      0.65      0.02      0.65      0.61      0.68   2845.53      1.00\n",
      "    p[1,35]      0.55      0.02      0.55      0.52      0.58   3887.00      1.00\n",
      "    p[1,36]      0.71      0.02      0.71      0.68      0.74   3725.20      1.00\n",
      "    p[1,37]      0.69      0.02      0.69      0.66      0.72   3664.87      1.00\n",
      "    p[1,38]      0.54      0.02      0.54      0.51      0.58   3895.22      1.00\n",
      "    p[1,39]      0.63      0.02      0.63      0.60      0.66   4602.20      1.00\n",
      "    p[1,40]      0.61      0.02      0.61      0.58      0.64   3681.94      1.00\n",
      "    p[1,41]      0.58      0.02      0.58      0.55      0.62   3366.98      1.00\n",
      "    p[1,42]      0.57      0.02      0.57      0.54      0.60   3390.98      1.00\n",
      "    p[1,43]      0.51      0.02      0.51      0.48      0.54   3545.61      1.00\n",
      "    p[1,44]      0.69      0.02      0.69      0.66      0.72   3393.05      1.00\n",
      "    p[1,45]      0.55      0.02      0.55      0.51      0.58   3040.11      1.00\n",
      "    p[1,46]      0.68      0.02      0.68      0.64      0.71   4420.68      1.00\n",
      "    p[1,47]      0.58      0.02      0.58      0.55      0.61   4550.60      1.00\n",
      "    p[1,48]      0.60      0.02      0.60      0.57      0.63   4398.25      1.00\n",
      "    p[1,49]      0.64      0.02      0.64      0.62      0.68   3365.77      1.00\n",
      "    p[1,50]      0.61      0.02      0.61      0.57      0.64   3746.08      1.00\n",
      "    p[1,51]      0.51      0.02      0.51      0.47      0.54   3598.19      1.00\n",
      "    p[1,52]      0.58      0.02      0.58      0.54      0.61   5221.72      1.00\n",
      "    p[1,53]      0.60      0.02      0.61      0.57      0.64   4357.66      1.00\n",
      "    p[1,54]      0.57      0.02      0.57      0.53      0.60   3733.15      1.00\n",
      "    p[1,55]      0.57      0.02      0.57      0.54      0.61   3519.12      1.00\n",
      "    p[1,56]      0.55      0.02      0.55      0.52      0.59   3276.70      1.00\n",
      "    p[1,57]      0.56      0.02      0.56      0.53      0.60   5726.98      1.00\n",
      "    p[1,58]      0.58      0.02      0.58      0.55      0.62   4313.02      1.00\n",
      "    p[1,59]      0.58      0.02      0.58      0.55      0.61   3279.64      1.00\n",
      "    p[1,60]      0.58      0.02      0.58      0.54      0.61   5105.32      1.00\n",
      "    p[1,61]      0.59      0.02      0.59      0.55      0.62   4749.14      1.00\n",
      "    p[1,62]      0.48      0.02      0.48      0.45      0.51   4783.89      1.00\n",
      "    p[1,63]      0.56      0.02      0.56      0.53      0.60   3820.03      1.00\n",
      "    p[1,64]      0.74      0.02      0.74      0.71      0.77   3247.37      1.00\n",
      "    p[1,65]      0.61      0.02      0.61      0.58      0.65   5505.53      1.00\n",
      "    p[1,66]      0.48      0.02      0.48      0.45      0.52   3913.63      1.00\n",
      "    p[1,67]      0.76      0.02      0.76      0.73      0.79   4460.41      1.00\n",
      "    p[1,68]      0.61      0.02      0.61      0.58      0.64   3585.20      1.00\n",
      "    p[1,69]      0.48      0.02      0.48      0.45      0.52   3279.80      1.00\n",
      "    p[1,70]      0.64      0.02      0.64      0.60      0.67   4300.22      1.00\n",
      "    p[1,71]      0.60      0.02      0.60      0.57      0.63   4512.46      1.00\n",
      "    p[1,72]      0.57      0.02      0.57      0.54      0.61   4127.26      1.00\n",
      "    p[1,73]      0.54      0.02      0.54      0.50      0.57   3908.08      1.00\n",
      "    p[1,74]      0.75      0.02      0.75      0.72      0.78   3251.25      1.00\n",
      "    p[1,75]      0.66      0.02      0.66      0.63      0.69   4221.04      1.00\n",
      "    p[1,76]      0.58      0.02      0.58      0.55      0.62   5714.53      1.00\n",
      "    p[1,77]      0.65      0.02      0.65      0.62      0.69   6246.49      1.00\n",
      "    p[1,78]      0.54      0.02      0.54      0.50      0.57   4821.12      1.00\n",
      "    p[1,79]      0.65      0.02      0.65      0.61      0.68   2982.65      1.00\n",
      "    p[1,80]      0.43      0.02      0.43      0.40      0.46   3944.36      1.00\n",
      "    p[1,81]      0.62      0.02      0.62      0.59      0.66   3957.55      1.00\n",
      "    p[1,82]      0.68      0.02      0.68      0.65      0.71   4231.89      1.00\n",
      "    p[1,83]      0.45      0.02      0.45      0.42      0.48   4561.95      1.00\n",
      "    p[1,84]      0.47      0.02      0.47      0.43      0.50   3147.21      1.00\n",
      "    p[1,85]      0.57      0.02      0.57      0.54      0.61   4552.69      1.00\n",
      "    p[1,86]      0.54      0.02      0.54      0.51      0.58   5145.76      1.00\n",
      "    p[1,87]      0.60      0.02      0.60      0.57      0.64   3551.42      1.00\n",
      "    p[1,88]      0.72      0.02      0.72      0.69      0.75   4110.56      1.00\n",
      "    p[1,89]      0.65      0.02      0.65      0.62      0.68   4407.35      1.00\n",
      "    p[1,90]      0.56      0.02      0.56      0.52      0.59   2427.54      1.00\n",
      "    p[1,91]      0.69      0.02      0.69      0.66      0.72   3375.50      1.00\n",
      "    p[1,92]      0.63      0.02      0.63      0.60      0.67   3767.43      1.00\n",
      "    p[1,93]      0.52      0.02      0.52      0.49      0.56   3196.42      1.00\n",
      "    p[1,94]      0.75      0.02      0.75      0.72      0.78   3727.75      1.00\n",
      "    p[1,95]      0.53      0.02      0.53      0.49      0.56   3859.02      1.00\n",
      "    p[1,96]      0.70      0.02      0.70      0.67      0.73   3236.96      1.00\n",
      "    p[1,97]      0.62      0.02      0.62      0.59      0.65   3444.66      1.00\n",
      "    p[1,98]      0.76      0.02      0.76      0.73      0.79   4307.99      1.00\n",
      "    p[1,99]      0.66      0.02      0.67      0.63      0.70   5937.51      1.00\n",
      "\n",
      "Number of divergences: 0\n",
      "Time Taken: 261.2399624288082\n"
     ]
    }
   ],
   "source": [
    "%run bg_lima.py '/data/notebooks/jupyterlab-biascorrlab/data/Lima2021/Land_Only_Distributed_Observations_100/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "451d4bb3-cec8-4eb4-8d2a-ba92e15bee09",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialisation before MLE\n",
      "Alpha\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-13 15:35:46.987998: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-13 15:35:46.988062: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>name                   </th><th>class    </th><th>transform       </th><th>prior  </th><th>trainable  </th><th>shape  </th><th>dtype  </th><th>value              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GPR.kernel.variance    </td><td>Parameter</td><td>Softplus        </td><td>       </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[0.5]              </td></tr>\n",
       "<tr><td>GPR.kernel.lengthscales</td><td>Parameter</td><td>Softplus        </td><td>       </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[1.]               </td></tr>\n",
       "<tr><td>GPR.likelihood.variance</td><td>Parameter</td><td>Softplus + Shift</td><td>       </td><td>True       </td><td>()     </td><td>float64</td><td>0.49999999999999994</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>name                   </th><th>class    </th><th>transform       </th><th>prior  </th><th>trainable  </th><th>shape  </th><th>dtype  </th><th>value              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GPR.kernel.variance    </td><td>Parameter</td><td>Softplus        </td><td>       </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[0.5]              </td></tr>\n",
       "<tr><td>GPR.kernel.lengthscales</td><td>Parameter</td><td>Softplus        </td><td>       </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[1.]               </td></tr>\n",
       "<tr><td>GPR.likelihood.variance</td><td>Parameter</td><td>Softplus + Shift</td><td>       </td><td>True       </td><td>()     </td><td>float64</td><td>0.49999999999999994</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-13 15:35:47.062654: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE Estimation of Alpha Parameter\n",
      "Step 0: loss -133.07817530251108\n",
      "Step 50: loss -127.00778250977572\n",
      "Step 100: loss -120.79636593086876\n",
      "Step 150: loss -117.93315481022063\n",
      "Step 200: loss -117.04985898379695\n",
      "Step 250: loss -116.67336869098915\n",
      "Step 300: loss -116.45832044128244\n",
      "Step 350: loss -116.31875463043176\n",
      "Step 400: loss -116.2244795598609\n",
      "Step 450: loss -116.159814087538\n",
      "Step 500: loss -116.11422793024786\n",
      "Step 550: loss -116.08059807276246\n",
      "Step 600: loss -116.05455884866652\n",
      "Step 650: loss -116.03361380484986\n",
      "Step 700: loss -116.0163186813761\n",
      "Step 750: loss -116.0017743877618\n",
      "Step 800: loss -115.98937355187897\n",
      "Step 850: loss -115.97868093298595\n",
      "Step 900: loss -115.9693729028265\n",
      "Step 950: loss -115.9612028477496\n",
      "Step 1000: loss -115.95397931722246\n",
      "Step 1050: loss -115.9475513680033\n",
      "Step 1100: loss -115.94179838044452\n",
      "Step 1150: loss -115.93662280858462\n",
      "Step 1200: loss -115.93194491793147\n",
      "Step 1250: loss -115.92769890155307\n",
      "Step 1300: loss -115.9238299704258\n",
      "Step 1350: loss -115.92029214413695\n",
      "Step 1400: loss -115.91704655273534\n",
      "Step 1450: loss -115.91406011680937\n",
      "Step 1500: loss -115.91130451098196\n",
      "Step 1550: loss -115.90875534225397\n",
      "Step 1600: loss -115.90639149295535\n",
      "Step 1650: loss -115.90419459107089\n",
      "Step 1700: loss -115.90214858002614\n",
      "Step 1750: loss -115.90023936680862\n",
      "Step 1800: loss -115.8984545322734\n",
      "Step 1850: loss -115.89678309118202\n",
      "Step 1900: loss -115.89521529229287\n",
      "Step 1950: loss -115.89374245091902\n",
      "MLE Estimation of P Parameter\n",
      "Step 0: loss -71.84709842628683\n",
      "Step 50: loss 39.127211395029875\n",
      "Step 100: loss 80.19264483284897\n",
      "Step 150: loss 89.4994917067163\n",
      "Step 200: loss 92.25357838665036\n",
      "Step 250: loss 93.55135019363686\n",
      "Step 300: loss 94.3308241512307\n",
      "Step 350: loss 94.86070430423618\n",
      "Step 400: loss 95.24818925118649\n",
      "Step 450: loss 95.54550077692576\n",
      "Step 500: loss 95.78156205684283\n",
      "Step 550: loss 95.97387349054583\n",
      "Step 600: loss 96.13372809748387\n",
      "Step 650: loss 96.2687786283577\n",
      "Step 700: loss 96.38441432523229\n",
      "Step 750: loss 96.48455021423436\n",
      "Step 800: loss 96.57210436026301\n",
      "Step 850: loss 96.6492992721295\n",
      "Step 900: loss 96.71785926558769\n",
      "Step 950: loss 96.77914371292903\n",
      "Step 1000: loss 96.8342393962472\n",
      "Step 1050: loss 96.88402598825965\n",
      "Step 1100: loss 96.92922341430503\n",
      "Step 1150: loss 96.97042671803715\n",
      "Step 1200: loss 97.00813213420548\n",
      "Step 1250: loss 97.04275686278783\n",
      "Step 1300: loss 97.07465425826175\n",
      "Step 1350: loss 97.10412563270938\n",
      "Step 1400: loss 97.13142952492984\n",
      "Step 1450: loss 97.15678905031331\n",
      "Step 1500: loss 97.18039778098637\n",
      "Step 1550: loss 97.20242448896286\n",
      "Step 1600: loss 97.22301700143396\n",
      "Step 1650: loss 97.24230535667505\n",
      "Step 1700: loss 97.2604044045722\n",
      "Step 1750: loss 97.27741596277517\n",
      "Step 1800: loss 97.29343061481063\n",
      "Step 1850: loss 97.30852921775596\n",
      "Step 1900: loss 97.32278417289945\n",
      "Step 1950: loss 97.33626050182014\n",
      "Model Summary after MLE\n",
      "Alpha\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>name                   </th><th>class    </th><th>transform       </th><th>prior  </th><th>trainable  </th><th>shape  </th><th>dtype  </th><th>value              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GPR.kernel.variance    </td><td>Parameter</td><td>Softplus        </td><td>       </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[1.07104459]       </td></tr>\n",
       "<tr><td>GPR.kernel.lengthscales</td><td>Parameter</td><td>Softplus        </td><td>       </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[0.20636786]       </td></tr>\n",
       "<tr><td>GPR.likelihood.variance</td><td>Parameter</td><td>Softplus + Shift</td><td>       </td><td>True       </td><td>()     </td><td>float64</td><td>0.00485989912639067</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>name                   </th><th>class    </th><th>transform       </th><th>prior  </th><th>trainable  </th><th>shape  </th><th>dtype  </th><th>value                 </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GPR.kernel.variance    </td><td>Parameter</td><td>Softplus        </td><td>       </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[0.28298265]          </td></tr>\n",
       "<tr><td>GPR.kernel.lengthscales</td><td>Parameter</td><td>Softplus        </td><td>       </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[5.48364254]          </td></tr>\n",
       "<tr><td>GPR.likelihood.variance</td><td>Parameter</td><td>Softplus + Shift</td><td>       </td><td>True       </td><td>()     </td><td>float64</td><td>0.00023227096119972433</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCMC Estimation of Parameters\n",
      "Model Initialisation before MCMC\n",
      "Alpha\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>name                   </th><th>class    </th><th>transform       </th><th>prior  </th><th>trainable  </th><th>shape  </th><th>dtype  </th><th>value              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GPR.kernel.variance    </td><td>Parameter</td><td>Softplus        </td><td>Gamma  </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[1.07104459]       </td></tr>\n",
       "<tr><td>GPR.kernel.lengthscales</td><td>Parameter</td><td>Softplus        </td><td>Gamma  </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[0.20636786]       </td></tr>\n",
       "<tr><td>GPR.likelihood.variance</td><td>Parameter</td><td>Softplus + Shift</td><td>Gamma  </td><td>True       </td><td>()     </td><td>float64</td><td>0.00485989912639067</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>name                   </th><th>class    </th><th>transform       </th><th>prior  </th><th>trainable  </th><th>shape  </th><th>dtype  </th><th>value                 </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GPR.kernel.variance    </td><td>Parameter</td><td>Softplus        </td><td>Gamma  </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[0.28298265]          </td></tr>\n",
       "<tr><td>GPR.kernel.lengthscales</td><td>Parameter</td><td>Softplus        </td><td>Gamma  </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[5.48364254]          </td></tr>\n",
       "<tr><td>GPR.likelihood.variance</td><td>Parameter</td><td>Softplus + Shift</td><td>Gamma  </td><td>True       </td><td>()     </td><td>float64</td><td>0.00023227096119972433</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary after MCMC\n",
      "Alpha\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>name                   </th><th>class    </th><th>transform       </th><th>prior  </th><th>trainable  </th><th>shape  </th><th>dtype  </th><th>value               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GPR.kernel.variance    </td><td>Parameter</td><td>Softplus        </td><td>Gamma  </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[0.97767104]        </td></tr>\n",
       "<tr><td>GPR.kernel.lengthscales</td><td>Parameter</td><td>Softplus        </td><td>Gamma  </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[0.15865485]        </td></tr>\n",
       "<tr><td>GPR.likelihood.variance</td><td>Parameter</td><td>Softplus + Shift</td><td>Gamma  </td><td>True       </td><td>()     </td><td>float64</td><td>0.001419485254252593</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>name                   </th><th>class    </th><th>transform       </th><th>prior  </th><th>trainable  </th><th>shape  </th><th>dtype  </th><th>value                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GPR.kernel.variance    </td><td>Parameter</td><td>Softplus        </td><td>Gamma  </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[0.10280291]         </td></tr>\n",
       "<tr><td>GPR.kernel.lengthscales</td><td>Parameter</td><td>Softplus        </td><td>Gamma  </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[1.62480282]         </td></tr>\n",
       "<tr><td>GPR.likelihood.variance</td><td>Parameter</td><td>Softplus + Shift</td><td>Gamma  </td><td>True       </td><td>()     </td><td>float64</td><td>8.088233713471549e-06</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run gp_lima.py '/data/notebooks/jupyterlab-biascorrlab/data/Lima2021/Land_Only_Distributed_Observations_100/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d3c1c6c-0200-49a0-ac43-50a5d2750d32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run qm_lima.py '/data/notebooks/jupyterlab-biascorrlab/data/Lima2021/Land_Only_Distributed_Observations_100/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "229dc717-255f-4337-955a-b2648ea84b74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run bg_lima.py '/data/notebooks/jupyterlab-biascorrlab/data/Lima2021/Randomly_Distributed_Observations_100/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7927f332-a383-4c5f-bf93-b1a16342c865",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialisation before MLE\n",
      "Alpha\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>name                   </th><th>class    </th><th>transform       </th><th>prior  </th><th>trainable  </th><th>shape  </th><th>dtype  </th><th>value              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GPR.kernel.variance    </td><td>Parameter</td><td>Softplus        </td><td>       </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[0.5]              </td></tr>\n",
       "<tr><td>GPR.kernel.lengthscales</td><td>Parameter</td><td>Softplus        </td><td>       </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[1.]               </td></tr>\n",
       "<tr><td>GPR.likelihood.variance</td><td>Parameter</td><td>Softplus + Shift</td><td>       </td><td>True       </td><td>()     </td><td>float64</td><td>0.49999999999999994</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>name                   </th><th>class    </th><th>transform       </th><th>prior  </th><th>trainable  </th><th>shape  </th><th>dtype  </th><th>value              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GPR.kernel.variance    </td><td>Parameter</td><td>Softplus        </td><td>       </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[0.5]              </td></tr>\n",
       "<tr><td>GPR.kernel.lengthscales</td><td>Parameter</td><td>Softplus        </td><td>       </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[1.]               </td></tr>\n",
       "<tr><td>GPR.likelihood.variance</td><td>Parameter</td><td>Softplus + Shift</td><td>       </td><td>True       </td><td>()     </td><td>float64</td><td>0.49999999999999994</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE Estimation of Alpha Parameter\n",
      "Step 0: loss -128.75067053379254\n",
      "Step 50: loss -122.04119492046419\n",
      "Step 100: loss -115.21732744163138\n",
      "Step 150: loss -112.3749844275863\n",
      "Step 200: loss -111.37328391075324\n",
      "Step 250: loss -110.85934079016216\n",
      "Step 300: loss -110.54365923269887\n",
      "Step 350: loss -110.33583538472593\n",
      "Step 400: loss -110.19444010420142\n",
      "Step 450: loss -110.09471761843518\n",
      "Step 500: loss -110.02111871258836\n",
      "Step 550: loss -109.96442369597864\n",
      "Step 600: loss -109.91931736150448\n",
      "Step 650: loss -109.8826015290658\n",
      "Step 700: loss -109.85220119649183\n",
      "Step 750: loss -109.8266819159478\n",
      "Step 800: loss -109.80500990416445\n",
      "Step 850: loss -109.78641923266672\n",
      "Step 900: loss -109.77033049538744\n",
      "Step 950: loss -109.75629775639158\n",
      "Step 1000: loss -109.74397262261203\n",
      "Step 1050: loss -109.7330792741364\n",
      "Step 1100: loss -109.7233967432642\n",
      "Step 1150: loss -109.71474610781726\n",
      "Step 1200: loss -109.70698108521475\n",
      "Step 1250: loss -109.69998102306042\n",
      "Step 1300: loss -109.69364560632164\n",
      "Step 1350: loss -109.68789081241881\n",
      "Step 1400: loss -109.68264578582078\n",
      "Step 1450: loss -109.67785039859798\n",
      "Step 1500: loss -109.6734533285486\n",
      "Step 1550: loss -109.6694105319674\n",
      "Step 1600: loss -109.66568402026297\n",
      "Step 1650: loss -109.66224087263708\n",
      "Step 1700: loss -109.659052433713\n",
      "Step 1750: loss -109.65609365720694\n",
      "Step 1800: loss -109.65334256577147\n",
      "Step 1850: loss -109.65077980389296\n",
      "Step 1900: loss -109.64838826580403\n",
      "Step 1950: loss -109.64615278424534\n",
      "MLE Estimation of P Parameter\n",
      "Step 0: loss -72.87296991011851\n",
      "Step 50: loss 38.56603145340688\n",
      "Step 100: loss 80.74921173455958\n",
      "Step 150: loss 95.23528571316888\n",
      "Step 200: loss 101.44742493187434\n",
      "Step 250: loss 104.59235564251446\n",
      "Step 300: loss 106.41217593419492\n",
      "Step 350: loss 107.57542379710668\n",
      "Step 400: loss 108.3750744327439\n",
      "Step 450: loss 108.95528350600011\n",
      "Step 500: loss 109.39395008023348\n",
      "Step 550: loss 109.73643329548409\n",
      "Step 600: loss 110.0107756894607\n",
      "Step 650: loss 110.23517958660878\n",
      "Step 700: loss 110.42194460257139\n",
      "Step 750: loss 110.57966598301951\n",
      "Step 800: loss 110.71452382931909\n",
      "Step 850: loss 110.83107150287054\n",
      "Step 900: loss 110.93273543518995\n",
      "Step 950: loss 111.02214203832855\n",
      "Step 1000: loss 111.10133745291225\n",
      "Step 1050: loss 111.17193886588436\n",
      "Step 1100: loss 111.23524096144018\n",
      "Step 1150: loss 111.29229225421727\n",
      "Step 1200: loss 111.34395077442085\n",
      "Step 1250: loss 111.39092532546337\n",
      "Step 1300: loss 111.43380648566972\n",
      "Step 1350: loss 111.47309020440656\n",
      "Step 1400: loss 111.50919597382722\n",
      "Step 1450: loss 111.54248097497606\n",
      "Step 1500: loss 111.57325120007016\n",
      "Step 1550: loss 111.60177027804357\n",
      "Step 1600: loss 111.62826653754797\n",
      "Step 1650: loss 111.65293870434837\n",
      "Step 1700: loss 111.6759605312065\n",
      "Step 1750: loss 111.69748458631568\n",
      "Step 1800: loss 111.71764537329062\n",
      "Step 1850: loss 111.73656191629573\n",
      "Step 1900: loss 111.7543399142292\n",
      "Step 1950: loss 111.77107354548264\n",
      "Model Summary after MLE\n",
      "Alpha\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>name                   </th><th>class    </th><th>transform       </th><th>prior  </th><th>trainable  </th><th>shape  </th><th>dtype  </th><th>value                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GPR.kernel.variance    </td><td>Parameter</td><td>Softplus        </td><td>       </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[0.98560731]         </td></tr>\n",
       "<tr><td>GPR.kernel.lengthscales</td><td>Parameter</td><td>Softplus        </td><td>       </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[0.25238743]         </td></tr>\n",
       "<tr><td>GPR.likelihood.variance</td><td>Parameter</td><td>Softplus + Shift</td><td>       </td><td>True       </td><td>()     </td><td>float64</td><td>0.0016453973216179031</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>name                   </th><th>class    </th><th>transform       </th><th>prior  </th><th>trainable  </th><th>shape  </th><th>dtype  </th><th>value                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GPR.kernel.variance    </td><td>Parameter</td><td>Softplus        </td><td>       </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[0.17292935]         </td></tr>\n",
       "<tr><td>GPR.kernel.lengthscales</td><td>Parameter</td><td>Softplus        </td><td>       </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[5.01966375]         </td></tr>\n",
       "<tr><td>GPR.likelihood.variance</td><td>Parameter</td><td>Softplus + Shift</td><td>       </td><td>True       </td><td>()     </td><td>float64</td><td>7.721270366247367e-05</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCMC Estimation of Parameters\n",
      "Model Initialisation before MCMC\n",
      "Alpha\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>name                   </th><th>class    </th><th>transform       </th><th>prior  </th><th>trainable  </th><th>shape  </th><th>dtype  </th><th>value                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GPR.kernel.variance    </td><td>Parameter</td><td>Softplus        </td><td>Gamma  </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[0.98560731]         </td></tr>\n",
       "<tr><td>GPR.kernel.lengthscales</td><td>Parameter</td><td>Softplus        </td><td>Gamma  </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[0.25238743]         </td></tr>\n",
       "<tr><td>GPR.likelihood.variance</td><td>Parameter</td><td>Softplus + Shift</td><td>Gamma  </td><td>True       </td><td>()     </td><td>float64</td><td>0.0016453973216179031</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>name                   </th><th>class    </th><th>transform       </th><th>prior  </th><th>trainable  </th><th>shape  </th><th>dtype  </th><th>value                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GPR.kernel.variance    </td><td>Parameter</td><td>Softplus        </td><td>Gamma  </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[0.17292935]         </td></tr>\n",
       "<tr><td>GPR.kernel.lengthscales</td><td>Parameter</td><td>Softplus        </td><td>Gamma  </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[5.01966375]         </td></tr>\n",
       "<tr><td>GPR.likelihood.variance</td><td>Parameter</td><td>Softplus + Shift</td><td>Gamma  </td><td>True       </td><td>()     </td><td>float64</td><td>7.721270366247367e-05</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary after MCMC\n",
      "Alpha\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>name                   </th><th>class    </th><th>transform       </th><th>prior  </th><th>trainable  </th><th>shape  </th><th>dtype  </th><th>value                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GPR.kernel.variance    </td><td>Parameter</td><td>Softplus        </td><td>Gamma  </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[0.89350875]         </td></tr>\n",
       "<tr><td>GPR.kernel.lengthscales</td><td>Parameter</td><td>Softplus        </td><td>Gamma  </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[0.19420337]         </td></tr>\n",
       "<tr><td>GPR.likelihood.variance</td><td>Parameter</td><td>Softplus + Shift</td><td>Gamma  </td><td>True       </td><td>()     </td><td>float64</td><td>0.0004984492879250895</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>name                   </th><th>class    </th><th>transform       </th><th>prior  </th><th>trainable  </th><th>shape  </th><th>dtype  </th><th>value                 </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GPR.kernel.variance    </td><td>Parameter</td><td>Softplus        </td><td>Gamma  </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[0.07087176]          </td></tr>\n",
       "<tr><td>GPR.kernel.lengthscales</td><td>Parameter</td><td>Softplus        </td><td>Gamma  </td><td>True       </td><td>(1,)   </td><td>float64</td><td>[1.72884088]          </td></tr>\n",
       "<tr><td>GPR.likelihood.variance</td><td>Parameter</td><td>Softplus + Shift</td><td>Gamma  </td><td>True       </td><td>()     </td><td>float64</td><td>3.0705888659792883e-06</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run gp_lima.py '/data/notebooks/jupyterlab-biascorrlab/data/Lima2021/Randomly_Distributed_Observations_100/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "395a9bad-c896-4531-8b48-0df31adc429f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1050 [00:00<?, ?it/s]/data/notebooks/jupyterlab-biascorrlab/scripts/qm_lima.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_all_daily_test['ecdf']=ecdf(data) #updating ecdf column\n",
      "/data/notebooks/jupyterlab-biascorrlab/scripts/qm_lima.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_all_daily_test['adjusted_ecdf']=df_all_daily_test['ecdf']\n",
      "/data/conda/bc/lib/python3.9/site-packages/pandas/core/frame.py:7511: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[col] = expressions.where(mask, this, that)\n",
      "/data/notebooks/jupyterlab-biascorrlab/scripts/qm_lima.py:114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nonzeros_df['corrected_prsn']=stats.gamma.ppf((nonzeros_df['ecdf']-(1-obs_p))/(obs_p), a=obs_alpha,loc=0, scale=obs_scale)\n",
      "/data/notebooks/jupyterlab-biascorrlab/scripts/qm_lima.py:121: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_all_daily_test['corrected_prsn']=df_all_daily_test['corrected_prsn']/10**5\n",
      "  1%|          | 6/1050 [00:00<01:14, 14.09it/s]/data/notebooks/jupyterlab-biascorrlab/scripts/qm_lima.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nonzero_values_to_convert['corrected_prsn']=0\n",
      "  3%|▎         | 32/1050 [00:02<01:10, 14.48it/s]/data/notebooks/jupyterlab-biascorrlab/scripts/qm_lima.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nonzeros_not_converted['corrected_prsn']=stats.gamma.ppf((nonzeros_not_converted['ecdf']-(1-obs_p))/(obs_p), a=obs_alpha,loc=0, scale=obs_scale)\n",
      "100%|██████████| 1050/1050 [01:02<00:00, 16.89it/s]\n"
     ]
    }
   ],
   "source": [
    "%run qm_lima.py '/data/notebooks/jupyterlab-biascorrlab/data/Lima2021/Randomly_Distributed_Observations_100/'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bc",
   "language": "python",
   "name": "bc-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
