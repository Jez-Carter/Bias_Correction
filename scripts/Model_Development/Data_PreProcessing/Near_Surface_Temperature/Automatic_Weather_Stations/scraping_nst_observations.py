import numpy as np
import pandas as pd
import requests
from bs4 import BeautifulSoup
import tabula

base_path = '/home/jez/'
pdf_path = f'{base_path}Bias_Correction/'
outpath = f'{base_path}data/ProcessedData/NST_Observations.nc'

### scraping csvs from https://amrdcdata.ssec.wisc.edu/dataset/antaws-dataset

source = requests.get('https://amrdcdata.ssec.wisc.edu/dataset/antaws-dataset')
soup = BeautifulSoup(source.content)
div = soup.find_all('div', {'class':'dropdown btn-group'})

station_names = []
hrefs = []

for a in soup.find_all('a', href=True):
    if all(x in a['href'] for x in ['csv','day']):
        if any(href == a['href'] for href in hrefs):
            continue
        station_name = a['href'].split('/')[-1].split('_day.csv')[0]
        station_names.append(station_name)
        hrefs.append(a['href'])   
        
dfs = []
for station_name,href in zip(station_names,hrefs):
    df = pd.read_csv(f'{href}',encoding_errors='ignore')
    df['Station']=station_name
    dfs.append(df)
df_all = pd.concat(dfs)
df_all = df_all.reset_index(drop=True)

### scraping locations from supplementary table from paper (https://essd.copernicus.org/preprints/essd-2022-241/essd-2022-241-supplement.pdf)

locations_dfs = tabula.read_pdf(pdf_path, pages='6-10')
columns = locations_dfs[0].columns
for df in locations_dfs[1:]:
    df.loc[-1] = df.columns
    df.sort_index(inplace=True) 
    df.columns = columns
    
locations_df = pd.concat(locations_dfs)
locations_df.reset_index(drop=True)

### renaming some stations for merge to work

locations_df['Station_Lower'] = [x.lower() for x in locations_df['Station']]
locations_df['Station_Lower'] = locations_df['Station_Lower'].str.replace(' ','-')
locations_df['Station_Lower'] = locations_df['Station_Lower'].str.replace('(','')
locations_df['Station_Lower'] = locations_df['Station_Lower'].str.replace(')','')
locations_df['Station_Lower'] = locations_df['Station_Lower'].str.replace('!','')

locations_df['Station_Lower'] = locations_df['Station_Lower'].str.replace('marilyn-byrd-glacier','marilyn')
locations_df['Station_Lower'] = locations_df['Station_Lower'].replace('mt.erebus','mt.-erebus')
locations_df['Station_Lower'] = locations_df['Station_Lower'].replace('mt.fleming','mt.-fleming')
locations_df['Station_Lower'] = locations_df['Station_Lower'].replace('panda-south','panda_south')

### merging station data and location data

df_all_combined = locations_df.merge(df_all, how='outer', left_on='Station_Lower', right_on='Station')

### reformatting pandas dataframe into xarray dataset 

new_index = df_all_combined.groupby(['Station_Lower','Month']).cumcount()
ds_all_combined = df_all_combined.set_index(['Station_Lower','Month',new_index]).to_xarray()

ds_all_combined_filtered = ds_all_combined.copy()

coords = ['Lat(°C)','Lon(°C)','Elevation(m)','Institution']
extra_vars_to_drop = ['Station_x','Station_y','Relative Humidity(%)','No.','Pressure(hPa)','wt_P','wt_T','Wind Speed(m/s)','wt_WS','Wind Direction','wt_WD','wt_RH']
ds_all_combined_filtered=ds_all_combined_filtered.drop(coords)
ds_all_combined_filtered=ds_all_combined_filtered.drop(extra_vars_to_drop)

for coord in coords:
    ds_all_combined_filtered = ds_all_combined_filtered.assign_coords({f"{coord}": ds_all_combined.isel(level_2=0).drop(['level_2'])[f'{coord}']})
    
ds_all_combined_filtered=ds_all_combined_filtered.rename({'Day':'Day_of_Month'})
ds_all_combined_filtered=ds_all_combined_filtered.rename({'level_2':'Day'})

Elevation_Nans = ds_all_combined_filtered['Elevation(m)'].data == '-'
ds_all_combined_filtered['Elevation(m)'].data[Elevation_Nans]=np.nan

### saving reformatted dataset

ds_all_combined_filtered.to_netcdf(outpath)